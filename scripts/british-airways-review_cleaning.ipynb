{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Loguru logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add(\n",
    "    \"../logs/data_cleaning.log\",\n",
    "    rotation=\"5 MB\",\n",
    "    retention=\"10 days\",\n",
    "    level=\"INFO\",\n",
    "    enqueue=True,\n",
    "    backtrace=True,\n",
    "    diagnose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download necessary NLTK data files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_nltk_path = os.path.expanduser('~/nltk_data')\n",
    "nltk.data.path.append(global_nltk_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aliassaad/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/aliassaad/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aliassaad/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords', download_dir=global_nltk_path)\n",
    "nltk.download('punkt_tab', download_dir=global_nltk_path)\n",
    "nltk.download('wordnet', download_dir=global_nltk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access NLTK components directly through nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_tokenize = nltk.word_tokenize\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/british_airways_raw_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>type_of_traveller</th>\n",
       "      <th>seat_type</th>\n",
       "      <th>route</th>\n",
       "      <th>date_flown</th>\n",
       "      <th>rating</th>\n",
       "      <th>recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Who can trust BA to travel2</td>\n",
       "      <td>J C Albrecht</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Not Verified  |  The flight scheduled at 1840 ...</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>London to Istanbul</td>\n",
       "      <td>October 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"just another poor airline\"</td>\n",
       "      <td>Dennis Teifeld</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>✅  Trip Verified  |   I have been flying BA fo...</td>\n",
       "      <td>Couple Leisure</td>\n",
       "      <td>Business Class</td>\n",
       "      <td>San Francisco to Barcelona via London</td>\n",
       "      <td>October 2024</td>\n",
       "      <td>5</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"spent two hours trying to make contact with BA\"</td>\n",
       "      <td>Paul Mercer</td>\n",
       "      <td>2024-10-25</td>\n",
       "      <td>✅  Trip Verified  |   On arriving at Mexico Ai...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Business Class</td>\n",
       "      <td>Mexico City to London Heathrow</td>\n",
       "      <td>October 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"using another airline for future travel\"</td>\n",
       "      <td>M Stansfield</td>\n",
       "      <td>2024-10-24</td>\n",
       "      <td>✅  Trip Verified  |   I have flown British Air...</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>Business Class</td>\n",
       "      <td>Paris to Boston via London</td>\n",
       "      <td>July 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"oversold tickets on our flight\"</td>\n",
       "      <td>Claude Cahn</td>\n",
       "      <td>2024-10-22</td>\n",
       "      <td>Not Verified  | We bought tickets for a Geneva...</td>\n",
       "      <td>Family Leisure</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>Geneva to London</td>\n",
       "      <td>September 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title          author  \\\n",
       "0                      \"Who can trust BA to travel2    J C Albrecht   \n",
       "1                       \"just another poor airline\"  Dennis Teifeld   \n",
       "2  \"spent two hours trying to make contact with BA\"     Paul Mercer   \n",
       "3         \"using another airline for future travel\"    M Stansfield   \n",
       "4                  \"oversold tickets on our flight\"     Claude Cahn   \n",
       "\n",
       "         date                                            content  \\\n",
       "0  2024-10-31  Not Verified  |  The flight scheduled at 1840 ...   \n",
       "1  2024-10-31  ✅  Trip Verified  |   I have been flying BA fo...   \n",
       "2  2024-10-25  ✅  Trip Verified  |   On arriving at Mexico Ai...   \n",
       "3  2024-10-24  ✅  Trip Verified  |   I have flown British Air...   \n",
       "4  2024-10-22  Not Verified  | We bought tickets for a Geneva...   \n",
       "\n",
       "  type_of_traveller       seat_type                                  route  \\\n",
       "0      Solo Leisure   Economy Class                     London to Istanbul   \n",
       "1    Couple Leisure  Business Class  San Francisco to Barcelona via London   \n",
       "2          Business  Business Class         Mexico City to London Heathrow   \n",
       "3      Solo Leisure  Business Class             Paris to Boston via London   \n",
       "4    Family Leisure   Economy Class                       Geneva to London   \n",
       "\n",
       "       date_flown  rating recommended  \n",
       "0    October 2024       1          no  \n",
       "1    October 2024       5          no  \n",
       "2    October 2024       1          no  \n",
       "3       July 2024       1          no  \n",
       "4  September 2024       1          no  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary text from the 'content' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove both '✅ Trip Verified |' and 'Not Verified |' at the start of the text\n",
    "    text = re.sub(r\"(✅\\s*Trip\\s*Verified\\s*\\|\\s*|Not\\s*Verified\\s*\\|\\s*)\", \"\", text, flags=re.IGNORECASE)\n",
    "    # Remove any remaining punctuation and extra whitespace\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Keep only word characters and whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy to rpeserve the main data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the cleaning function to the 'content' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['content_cleaned'] = df['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Verified  |  The flight scheduled at 1840 ...</td>\n",
       "      <td>The flight scheduled at 1840 left 2hours 40 mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅  Trip Verified  |   I have been flying BA fo...</td>\n",
       "      <td>I have been flying BA for over 15 years I was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅  Trip Verified  |   On arriving at Mexico Ai...</td>\n",
       "      <td>On arriving at Mexico Airport we were told tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅  Trip Verified  |   I have flown British Air...</td>\n",
       "      <td>I have flown British Airways for many years an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not Verified  | We bought tickets for a Geneva...</td>\n",
       "      <td>We bought tickets for a Geneva London flight b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Not Verified  |  The flight scheduled at 1840 ...   \n",
       "1  ✅  Trip Verified  |   I have been flying BA fo...   \n",
       "2  ✅  Trip Verified  |   On arriving at Mexico Ai...   \n",
       "3  ✅  Trip Verified  |   I have flown British Air...   \n",
       "4  Not Verified  | We bought tickets for a Geneva...   \n",
       "\n",
       "                                     content_cleaned  \n",
       "0  The flight scheduled at 1840 left 2hours 40 mi...  \n",
       "1  I have been flying BA for over 15 years I was ...  \n",
       "2  On arriving at Mexico Airport we were told tha...  \n",
       "3  I have flown British Airways for many years an...  \n",
       "4  We bought tickets for a Geneva London flight b...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[['content', 'content_cleaned']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text, remove stopwords, and lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Processes the cleaned text by tokenizing, removing stopwords, and lemmatizing.\n",
    "    Returns both the processed text string and the list of tokens.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        words = word_tokenize(text.lower())\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "        processed_text = \" \".join(tokens)\n",
    "        return processed_text, tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing text: {e}\")\n",
    "        return \"\", []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess the 'content' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[['content_processed', 'tokens']] = df_clean['content_cleaned'].apply(\n",
    "            lambda x: pd.Series(process_text(x))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_cleaned</th>\n",
       "      <th>content_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>Dubai to London on 5th December Flight was ok ...</td>\n",
       "      <td>dubai london 5th december flight ok seat comfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>Gatwick to Fort Lauderdale Charging to choose ...</td>\n",
       "      <td>gatwick fort lauderdale charging choose seat e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        content_cleaned  \\\n",
       "736   Dubai to London on 5th December Flight was ok ...   \n",
       "1176  Gatwick to Fort Lauderdale Charging to choose ...   \n",
       "\n",
       "                                      content_processed  \n",
       "736   dubai london 5th december flight ok seat comfo...  \n",
       "1176  gatwick fort lauderdale charging choose seat e...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[['content_cleaned', 'content_processed']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'author', 'date', 'content', 'type_of_traveller', 'seat_type',\n",
       "       'route', 'date_flown', 'rating', 'recommended', 'content_cleaned',\n",
       "       'content_processed', 'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-01 22:57:36.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mDropped 0 reviews due to missing processed content.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "initial_count = len(df_clean)\n",
    "df_clean.dropna(subset=['content_processed', 'tokens'], inplace=True)\n",
    "final_count = len(df_clean)\n",
    "logger.info(f\"Dropped {initial_count - final_count} reviews due to missing processed content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.drop(columns=['content', 'content_cleaned'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tokens list to space-separated string for CSV compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['tokens_str'] = df_clean['tokens'].apply(lambda x: ' '.join(x))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "output_directory = \"../data/processed\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "df_clean[['content_processed', 'tokens_str']].to_csv(f\"{output_directory}/british_airways_processed_reviews.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Skytrax Venv",
   "language": "python",
   "name": "skytrax_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
